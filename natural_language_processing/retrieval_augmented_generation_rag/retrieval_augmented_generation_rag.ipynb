{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a natural language processing (NLP) technique that combines the strengths of retrieval-based and generation-based models. It enhances traditional language generation models by incorporating a retrieval mechanism to fetch relevant context from a large corpus of text before generating responses. This approach allows RAG models to produce more accurate and contextually relevant outputs.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "\n",
    "The RAG model generates answers using the following formula:\n",
    "\n",
    "Answer = $\\arg\\max_{a} P(a|q, C)$]\n",
    "\n",
    "Where:\n",
    "- Answer represents the answer generated by the model.\n",
    "- $\\arg\\max_{a}$ denotes the argument that maximizes the function with respect to \\( a \\).\n",
    "- $P(a|q, C)$ represents the probability of generating answer \\( a \\) given the query \\( q \\) and retrieved context \\( C \\).\n",
    "\n",
    "#### Business Scenario Example:\n",
    "\n",
    "In a cybersecurity context, suppose a security analyst wants to gather information about Zscaler, a cloud-based cybersecurity company, to understand its offerings and capabilities. Using RAG, the analyst can input a query such as \"What are the key features of Zscaler's cloud security platform?\" The RAG model then retrieves relevant information from large text datasets, such as the Stanford Question Answering Dataset, Wikipedia, or industry reports. Finally, based on the retrieved context, the RAG model generates a comprehensive answer that highlights the key features of Zscaler's platform.\n",
    "\n",
    "**Note:** This example illustrates a simplified scenario. In practice, RAG can leverage much larger text datasets, such as the Stanford Question Answering Dataset, Wikipedia, or domain-specific corpora, to generate accurate and informative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What were the key factors in Zscaler's growth and market success after its founding?\n",
      "Most similar passage: Zscaler has shown remarkable growth since its IPO, with its cloud security solutions being adopted by thousands of companies worldwide, including several Fortune 500 firms. The company's focus on innovation and expanding its security platform has driven its market valuation significantly.\n",
      "Similarity score: 0.6895880103111267\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRQuestionEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "import logging\n",
    "import warnings\n",
    "# Suppress warnings from the Transformers library\n",
    "transformers.logging.set_verbosity_error()\n",
    "# Suppress all warnings (including FutureWarning, etc.)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Optionally, set the Python logging module to ERROR level\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Load the tokenizers and models\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Define the updated question\n",
    "question = \"What were the key factors in Zscaler's growth and market success after its founding?\"\n",
    "\n",
    "# Define the updated passages\n",
    "passages = [\n",
    "    \"Zscaler is a cloud-based cybersecurity company that offers a comprehensive platform for secure internet access and digital transformation. It provides cloud security solutions designed to protect users, data, and applications from various cyber threats, regardless of their location or device. Zscaler's platform operates by routing traffic through its global network of data centers, where it applies security policies and performs real-time threat analysis to detect and prevent cyber attacks. Key features of Zscaler include web security, cloud application security, zero trust network access, data protection, and threat intelligence. By offering a unified and scalable security solution, Zscaler helps organizations enhance their security posture while enabling secure and efficient digital experiences for users.\",\n",
    "    \"Founded in 2007 by Jay Chaudhry and K. Kailash, Zscaler's mission was to transform network security for the modern cloud era. The company went public in March 2018, highlighting its rapid growth and the increasing demand for cloud-native security solutions.\",\n",
    "    \"Zscaler has shown remarkable growth since its IPO, with its cloud security solutions being adopted by thousands of companies worldwide, including several Fortune 500 firms. The company's focus on innovation and expanding its security platform has driven its market valuation significantly.\",\n",
    "    \"Zscaler invests heavily in research and development to stay ahead of emerging cybersecurity threats. Its proprietary technologies, such as the Zscaler Zero Trust Exchange, leverage artificial intelligence and machine learning to provide enhanced security measures.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the question\n",
    "question_inputs = question_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output\n",
    "\n",
    "# Tokenize and encode each passage\n",
    "passage_embeddings = []\n",
    "for passage in passages:\n",
    "    passage_inputs = context_tokenizer(passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        passage_embedding = context_encoder(**passage_inputs).pooler_output\n",
    "        passage_embeddings.append(passage_embedding)\n",
    "\n",
    "# Stack embeddings for similarity calculation\n",
    "passage_embeddings = torch.stack(passage_embeddings).squeeze(1)\n",
    "\n",
    "# Calculate cosine similarity scores between question and each passage embedding\n",
    "similarity_scores = torch.nn.functional.cosine_similarity(question_embedding, passage_embeddings, dim=-1)\n",
    "\n",
    "# Identify the most similar passage\n",
    "max_similarity_score, max_similarity_idx = torch.max(similarity_scores, dim=0)\n",
    "\n",
    "# Output the most relevant passage and its similarity score\n",
    "print(\"Question:\", question)\n",
    "print(\"Most similar passage:\", passages[max_similarity_idx.item()])\n",
    "print(\"Similarity score:\", max_similarity_score.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
